# EAT

Reproduction is straight forward and implemented by the authors, so I have not added anything in that front. I wanted to see how much improvement has batch hard soft margin loss achieved over using the classic triplet loss. Using the traditional method has one clear disadvantage - we cannot utilize the CATH heirarchies for the data triplets. In [work.ipynb](https://github.com/Chokerino/EAT/blob/main/work.ipynb), I have replicated the t-SNE results and generated t-SNE plots for triplet loss. 

<figure>
  <p align = "center">
  <img
  src="https://github.com/Chokerino/EAT/blob/main/new_plots/orignal_embeddings.png"
  alt="Original Embeddings" width="500" height="500" >
  </p>
  <p align = "center">
	Embeddings generated by ProtT5
</p>
</figure>

<figure>
  <p align = "center">
  <img
  src="https://github.com/Chokerino/EAT/blob/main/new_plots/orignal_model_embeddings.png"
  alt="Original Embeddings" width="500" height="500" >
  </p>
  <p align = "center">
	Original Model Soft Margin Batch Hard Loss Embeddings
</p>
</figure>

<figure>
  <p align = "center">
  <img
  src="https://github.com/Chokerino/EAT/blob/main/new_plots/triplet_loss_embeddings.png"
  alt="Original Embeddings" width="500" height="500" >
  </p>
  <p align = "center">
	Triplet Loss Embeddings
</p>
</figure>

The Soft Margin Batch Hard Loss provides significant improvement over the Triplet Loss. Overall there is not much I can add to this right now.

Doubts :
 - Unsure on why the mask is being used the way it is. Initially it will mask identical heirarchies for each data point but the for each heirarchy the mask is updated using ![image](https://user-images.githubusercontent.com/43617111/168583462-f0eb76ea-43f2-4ac2-af18-92a7d64e46be.png) and not performing this step fails the training but I'm not sure why. Also, I'm not sure why we need two different masks for calculating the anchor-positive and anchor-negative distances because some implementations only use one.



<hr style="border:5px solid gray">


# Train your own network

The following steps will allow you to replicate the training of ProtTucker:
First, clone this repository and install dependencies:

```sh
git clone https://github.com/Rostlab/EAT.git
```

Next, download pre-computed embeddings used in the paper (ProtT5, ProtBERT, ESM-1b, ProSE: 5.5GB in total) to data/ProtTucker and unzip them. Also, download CATH annotations used for training.

```sh
wget -P data/ProtTucker/ https://rostlab.org/~deepppi/prottucker_training_embeddings.tar.gz
tar -xvf data/ProtTucker/prottucker_training_embeddings.tar.gz -C data/ProtTucker/ --strip-components 1
wget -P data/ProtTucker https://rostlab.org/~deepppi/cath-domain-list.txt
```

Finally, start training by running the training script:

```sh
python train_prottucker.py
```

By default, this will train ProtTucker as reported in the paper using embeddings from ProtT5.
In order to change the input embeddings, you can either replace the file name for 'embedding_p' OR compute your own embeddings (supported input format: H5).

# Reference (Pre-print)

```
@article {Heinzinger2021.11.14.468528,
	author = {Heinzinger, Michael and Littmann, Maria and Sillitoe, Ian and Bordin, Nicola and Orengo, Christine and Rost, Burkhard},
	title = {Contrastive learning on protein embeddings enlightens midnight zone at lightning speed},
	year = {2021},
	doi = {10.1101/2021.11.14.468528},
	URL = {https://www.biorxiv.org/content/early/2021/11/15/2021.11.14.468528},
	journal = {bioRxiv}
}
```
